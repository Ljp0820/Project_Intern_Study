{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gravity = 10\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras import layers, Model\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env=gym.make('Pendulum-v1', render_mode='human')\n",
    "\n",
    "class DQN(Model):\n",
    "\n",
    "    def __init__(self, input_state, output_state):\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        self.d1=layers.Dense(32, input_dim=input_state, activation='relu')\n",
    "        self.d2=layers.Dense(16, activation='relu')\n",
    "        self.d3=layers.Dense(16, activation='relu')\n",
    "        self.d4=layers.Dense(output_state, activation='linear')\n",
    "        self.optimizer=tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        x=self.d1(x)\n",
    "        x=self.d2(x)\n",
    "        x=self.d3(x)\n",
    "\n",
    "        return self.d4(x)\n",
    "    \n",
    "class Agent():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.state_size=3\n",
    "        self.action_size=41\n",
    "        self.eps=1.0\n",
    "        self.eps_decay=0.98\n",
    "        self.eps_min=0.1\n",
    "        self.batch_size=64\n",
    "        self.learning_rate=0.001\n",
    "        self.discount_factor=0.99\n",
    "        self.memory=[]\n",
    "        self.model=DQN(self.state_size, self.action_size)      \n",
    "\n",
    "    def update_eps(self):\n",
    "\n",
    "        self.eps=max(self.eps*self.eps_decay, self.eps_min)\n",
    "\n",
    "    def eps_greedy(self, state):\n",
    "        \n",
    "        if self.eps < np.random.rand():\n",
    "            return np.random.uniform(-2,2)\n",
    "        else:\n",
    "            act=np.argmax(self.model.call(np.array([state])))\n",
    "            return -2+act*0.1\n",
    "    \n",
    "    def append_sample(self, n_state, action, reward, termination, state):\n",
    "\n",
    "        self.memory.append((n_state, action, reward, termination, state))\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        if len(self.memory)<1000:\n",
    "            return\n",
    "\n",
    "        if len(self.memory)>20000:\n",
    "            del self.memory[0]\n",
    "\n",
    "        if len(self.memory)>self.batch_size:\n",
    "\n",
    "            mini_batch=random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            n_states=np.array([x[0] for x in mini_batch])\n",
    "            actions=np.array([x[1] for x in mini_batch])\n",
    "            rewards=np.array([x[2] for x in mini_batch])\n",
    "            terminations=np.array([x[3] for x in mini_batch])\n",
    "            states=np.array([x[4] for x in mini_batch])\n",
    "\n",
    "            q_val=self.model.call(states).numpy()\n",
    "            n_q_val=self.model.call(n_states).numpy()\n",
    "\n",
    "            targets=q_val.copy()\n",
    "            targets[np.arange(len(rewards)), actions]=rewards+self.discount_factor*np.max(n_q_val, axis=1)*(1-terminations)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_val=self.model.call(states)\n",
    "                loss=tf.keras.losses.mse(targets, q_val)\n",
    "\n",
    "            gradients=tape.gradient(loss, self.model.trainable_variables)\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "env.reset()\n",
    "epi=1000\n",
    "agent=Agent()\n",
    "\n",
    "for i in range(epi):\n",
    "    \n",
    "    state, _  = env.reset()\n",
    "    agent.update_eps()\n",
    "    termination=False\n",
    "    step=0\n",
    "    max_reward=-20\n",
    "\n",
    "    while not termination and step<1000:\n",
    "\n",
    "        action = agent.eps_greedy(state)\n",
    "        n_state, reward, termination, _, _ =env.step([action])\n",
    "        action=int((action+2)*10)\n",
    "\n",
    "        if termination:\n",
    "            reward=1000\n",
    "\n",
    "        max_reward=max(reward, max_reward)\n",
    "\n",
    "        agent.append_sample(n_state, action, reward, termination, state)\n",
    "        agent.train_model()\n",
    "\n",
    "        state=n_state\n",
    "        step+=1\n",
    "\n",
    "    print(i+1, step, max_reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gravity = 10, state=41\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras import layers, Model\n",
    "import gymnasium as gym\n",
    "\n",
    "env=gym.make('Pendulum-v1', render_mode='human')\n",
    "\n",
    "class DQN(Model):\n",
    "\n",
    "    def __init__(self, input_state, output_state):\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        self.d1=layers.Dense(32, input_dim=input_state, activation='relu')\n",
    "        self.d2=layers.Dense(16, activation='relu')\n",
    "        self.d3=layers.Dense(16, activation='relu')\n",
    "        self.d4=layers.Dense(output_state, activation='linear')\n",
    "        self.optimizer=tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        x=self.d1(x)\n",
    "        x=self.d2(x)\n",
    "        x=self.d3(x)\n",
    "        return self.d4(x)\n",
    "    \n",
    "class Agent():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.state_size=3\n",
    "        self.action_size=41\n",
    "        self.eps=1.0\n",
    "        self.eps_decay=0.98\n",
    "        self.eps_min=0.1\n",
    "        self.batch_size=64\n",
    "        self.learning_rate=0.001\n",
    "        self.discount_factor=0.99\n",
    "        self.memory=[]\n",
    "        self.model=DQN(self.state_size, self.action_size)      \n",
    "\n",
    "    def update_eps(self):\n",
    "\n",
    "        self.eps=max(self.eps*self.eps_decay, self.eps_min)\n",
    "\n",
    "    def eps_greedy(self, state):\n",
    "        \n",
    "        if np.random.rand()<self.eps:\n",
    "            return np.random.uniform(-2,2)\n",
    "        else:\n",
    "            act=np.argmax(self.model.call(np.array([state])))\n",
    "            return -2+act*0.1\n",
    "    \n",
    "    def append_sample(self, n_state, action, reward, termination, state):\n",
    "\n",
    "        self.memory.append((n_state, action, reward, termination, state))\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        if len(self.memory)<1000:\n",
    "            return\n",
    "\n",
    "        if len(self.memory)>20000:\n",
    "            del self.memory[0]\n",
    "\n",
    "        if len(self.memory)>self.batch_size:\n",
    "\n",
    "            mini_batch=random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            n_states=np.array([x[0] for x in mini_batch])\n",
    "            actions=np.array([x[1] for x in mini_batch])\n",
    "            rewards=np.array([x[2] for x in mini_batch])\n",
    "            terminations=np.array([x[3] for x in mini_batch])\n",
    "            states=np.array([x[4] for x in mini_batch])\n",
    "\n",
    "            q_val=self.model.call(states).numpy()\n",
    "            n_q_val=self.model.call(n_states).numpy()\n",
    "\n",
    "            targets=q_val.copy()\n",
    "            targets[np.arange(len(rewards)), actions]=rewards+self.discount_factor*np.max(n_q_val, axis=1)*(1-terminations)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_val=self.model.call(states)\n",
    "                loss=tf.keras.losses.mse(targets, q_val)\n",
    "\n",
    "            gradients=tape.gradient(loss, self.model.trainable_variables)\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "env.reset()\n",
    "epi=100\n",
    "agent=Agent()\n",
    "memory_step=[]\n",
    "count=0\n",
    "\n",
    "for i in range(epi):\n",
    "    \n",
    "    state, _  = env.reset()\n",
    "    agent.update_eps()\n",
    "    termination=False\n",
    "    step=0\n",
    "    max_reward=-20\n",
    "    \n",
    "    while not termination and step<1000:\n",
    "\n",
    "        action = agent.eps_greedy(state)\n",
    "        n_state, reward, termination, _, _ =env.step([action])\n",
    "        action=int((action+2)*10)\n",
    "\n",
    "        if reward>=-0.015:\n",
    "            termination=True\n",
    "            memory_step.append(step)\n",
    "            count+=1\n",
    "\n",
    "        max_reward=max(reward, max_reward)\n",
    "\n",
    "        agent.append_sample(n_state, action, reward, termination, state)\n",
    "        agent.train_model()\n",
    "\n",
    "        state=n_state\n",
    "        step+=1\n",
    "\n",
    "    print(i+1, step, max_reward)\n",
    "\n",
    "agent.model.save_weights(\"./save_model/model\", save_format=\"tf\")\n",
    "env.close()\n",
    "print('성공횟수 : {}, 평균 step : {}'.format(count, (sum(memory_step)/count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gravity = 10, state=41\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras import layers, Model\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env=gym.make('Pendulum-v1', render_mode='human')\n",
    "\n",
    "class DQN(Model):\n",
    "\n",
    "    def __init__(self, input_state, output_state):\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        self.d1=layers.Dense(32, input_dim=input_state, activation='relu')\n",
    "        self.d2=layers.Dense(16, activation='relu')\n",
    "        self.d3=layers.Dense(16, activation='relu')\n",
    "        self.d4=layers.Dense(output_state, activation='linear')\n",
    "        self.optimizer=tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        x=self.d1(x)\n",
    "        x=self.d2(x)\n",
    "        x=self.d3(x)\n",
    "        return self.d4(x)\n",
    "    \n",
    "class Agent():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.state_size=3\n",
    "        self.action_size=41\n",
    "        self.eps=1.0\n",
    "        self.eps_decay=0.98\n",
    "        self.eps_min=0.1\n",
    "        self.batch_size=64\n",
    "        self.learning_rate=0.001\n",
    "        self.discount_factor=0.99\n",
    "        self.memory=[]\n",
    "        self.model=DQN(self.state_size, self.action_size)      \n",
    "\n",
    "    def update_eps(self):\n",
    "\n",
    "        self.eps=max(self.eps*self.eps_decay, self.eps_min)\n",
    "\n",
    "    def eps_greedy(self, state):\n",
    "        \n",
    "        if np.random.rand()<self.eps:\n",
    "            return np.random.uniform(-2,2)\n",
    "        else:\n",
    "            act=np.argmax(self.model.call(np.array([state])))\n",
    "            return -2+act*0.1\n",
    "    \n",
    "    def append_sample(self, n_state, action, reward, termination, state):\n",
    "\n",
    "        self.memory.append((n_state, action, reward, termination, state))\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        if len(self.memory)<1000:\n",
    "            return\n",
    "\n",
    "        if len(self.memory)>20000:\n",
    "            del self.memory[0]\n",
    "\n",
    "        if len(self.memory)>self.batch_size:\n",
    "\n",
    "            mini_batch=random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            n_states=np.array([x[0] for x in mini_batch])\n",
    "            actions=np.array([x[1] for x in mini_batch])\n",
    "            rewards=np.array([x[2] for x in mini_batch])\n",
    "            terminations=np.array([x[3] for x in mini_batch])\n",
    "            states=np.array([x[4] for x in mini_batch])\n",
    "\n",
    "            q_val=self.model.call(states).numpy()\n",
    "            n_q_val=self.model.call(n_states).numpy()\n",
    "\n",
    "            targets=q_val.copy()\n",
    "            targets[np.arange(len(rewards)), actions]=rewards+self.discount_factor*np.max(n_q_val, axis=1)*(1-terminations)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_val=self.model.call(states)\n",
    "                loss=tf.keras.losses.mse(targets, q_val)\n",
    "\n",
    "            gradients=tape.gradient(loss, self.model.trainable_variables)\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "env.reset()\n",
    "agent=Agent()\n",
    "\n",
    "agent.model.load_weights('./save_model/model')\n",
    "agent.eps=0.01\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    state, _  = env.reset()\n",
    "    termination=False\n",
    "    max_reward=-20\n",
    "    step=0\n",
    "    env.render()\n",
    "    \n",
    "    while not termination:\n",
    "\n",
    "        action = agent.eps_greedy(state)\n",
    "        n_state, reward, termination, _, _ =env.step([action])\n",
    "        action=int((action+2)*10)\n",
    "\n",
    "        if reward>=-0.015:\n",
    "            termination=True\n",
    "\n",
    "        max_reward=max(reward, max_reward)\n",
    "        state=n_state\n",
    "        step+=1\n",
    "\n",
    "    print(i+1, step, max_reward)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
