{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1000 -0.3372658\n",
      "2 1000 -0.37137675\n",
      "3 1000 -0.1896321\n",
      "4 1000 -0.3280672\n",
      "5 1000 -0.2809995\n",
      "6 1000 0.283267\n",
      "7 1000 -0.3078933\n",
      "8 1000 0.29938993\n",
      "9 1000 0.34483078\n",
      "10 1000 0.322477\n",
      "11 1000 0.34850666\n",
      "12 1000 0.10603861\n",
      "13 452 0.5024678\n",
      "14 708 0.50630116\n",
      "15 250 0.5004995\n",
      "16 373 0.52902514\n",
      "17 385 0.5169368\n",
      "18 310 0.50995207\n",
      "19 350 0.50906664\n",
      "20 333 0.52561474\n",
      "21 329 0.5203854\n",
      "22 417 0.51180446\n",
      "23 349 0.50001895\n",
      "24 240 0.50573015\n",
      "25 405 0.52636\n",
      "26 307 0.5124214\n",
      "27 250 0.50254935\n",
      "28 289 0.5185026\n",
      "29 400 0.50837106\n",
      "30 300 0.5048926\n",
      "31 316 0.5064211\n",
      "32 172 0.52098566\n",
      "33 258 0.50249016\n",
      "34 409 0.5009229\n",
      "35 232 0.5078359\n",
      "36 327 0.53479\n",
      "37 334 0.5076587\n",
      "38 467 0.52379936\n",
      "39 173 0.50611526\n",
      "40 165 0.502376\n",
      "41 180 0.5031163\n",
      "42 330 0.5278593\n",
      "43 294 0.50092334\n",
      "44 301 0.5186913\n",
      "45 261 0.5170992\n",
      "46 242 0.51891655\n",
      "47 202 0.51237524\n",
      "48 163 0.50928134\n",
      "49 148 0.5076828\n",
      "50 171 0.5140109\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, Model\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "env=gym.make(\"MountainCar-v0\")\n",
    "\n",
    "class DQN(Model):\n",
    "\n",
    "    def __init__(self, indim, outdim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.d1=layers.Dense(32, activation='relu', input_dim=indim)\n",
    "        self.d2=layers.Dense(16, activation='relu')\n",
    "        self.d3=layers.Dense(16, activation='relu')\n",
    "        self.d4=layers.Dense(outdim, activation='linear')\n",
    "        self.optimizer=tf.optimizers.Adam(0.001)\n",
    "\n",
    "    def call(self, x):\n",
    "        x=self.d1(x)\n",
    "        x=self.d2(x)\n",
    "        x=self.d3(x)\n",
    "        return self.d4(x)\n",
    "    \n",
    "class Agent():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size=2\n",
    "        self.action_size=3\n",
    "        self.eps=1.0\n",
    "        self.eps_decay=0.98\n",
    "        self.min_eps=0.1\n",
    "        self.batch_size=64\n",
    "        self.discount=0.99\n",
    "        self.learning_rate=0.001\n",
    "        self.memory=[]\n",
    "        self.model=DQN(self.input_size, self.action_size)\n",
    "\n",
    "    def update_eps(self):\n",
    "        self.eps=max(self.eps*self.eps_decay, self.min_eps)\n",
    "\n",
    "    def memory_update(self, n_state, action, reward, done, state):\n",
    "        self.memory.append((n_state, action, reward, done, state))\n",
    "\n",
    "    def get_act(self, state):\n",
    "        if np.random.rand() < self.eps:\n",
    "            return np.random.randint(0,3)\n",
    "        else:\n",
    "            return np.argmax(self.model.call(np.array([state])))\n",
    "        \n",
    "    def update_model(self):\n",
    "\n",
    "        if len(self.memory)<1000:\n",
    "            return\n",
    "\n",
    "        if len(self.memory)>20000:\n",
    "            del self.memory[0]\n",
    "\n",
    "        if len(self.memory)>self.batch_size:\n",
    "\n",
    "            mini_batch=random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            n_states=np.array([x[0] for x in mini_batch])\n",
    "            actions=np.array([x[1] for x in mini_batch])\n",
    "            rewards=np.array([x[2] for x in mini_batch])\n",
    "            dones=np.array([x[3] for x in mini_batch])\n",
    "            states=np.array([x[4] for x in mini_batch])\n",
    "\n",
    "            q_val=self.model.call(states).numpy()\n",
    "            n_q_val=self.model.call(n_states).numpy()\n",
    "\n",
    "            targets=q_val.copy()\n",
    "            targets[np.arange(len(rewards)), actions]=rewards+self.discount*np.max(n_q_val, axis=1)*(1-dones)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_val=self.model.call(states)\n",
    "                loss=tf.keras.losses.mse(targets, q_val)\n",
    "\n",
    "            gradients=tape.gradient(loss, self.model.trainable_variables)\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "model=DQN(2,3)\n",
    "agent=Agent()\n",
    "epi=500\n",
    "suc=[]\n",
    "\n",
    "for i in range(epi):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    agent.update_eps()\n",
    "    max_pos=state[0]\n",
    "    step=0\n",
    "    done=False\n",
    "\n",
    "    while not done and step<1000:\n",
    "\n",
    "        action=agent.get_act(state)\n",
    "\n",
    "        n_state, reward, done, trunc, _ = env.step(action)\n",
    "\n",
    "        if n_state[0]>max_pos:\n",
    "            max_pos=n_state[0]\n",
    "\n",
    "        if done:\n",
    "            reward=100\n",
    "            suc.append(step)\n",
    "        if action==2 and n_state[0]>state[0]:\n",
    "            reward=3\n",
    "        elif action==0 and n_state[0]<state[0]:\n",
    "            reward=3\n",
    "\n",
    "        agent.memory_update(n_state, action, reward, done, state)\n",
    "        agent.update_model()\n",
    "\n",
    "        state=n_state\n",
    "        step+=1\n",
    "\n",
    "    print(i+1, step, max_pos)\n",
    "\n",
    "    if len(suc)>5 and sum(suc[-5:])/5<=200:\n",
    "        agent.model.save_weights(\"./save_model/model\", save_format=\"tf\")\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 126\n",
      "2 127\n",
      "3 126\n",
      "4 125\n",
      "5 126\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, Model\n",
    "import gymnasium as gym\n",
    "import random\n",
    "\n",
    "env=gym.make(\"MountainCar-v0\", render_mode='human')\n",
    "\n",
    "class DQN(Model):\n",
    "\n",
    "    def __init__(self, indim, outdim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.d1=layers.Dense(32, activation='relu', input_dim=indim)\n",
    "        self.d2=layers.Dense(16, activation='relu')\n",
    "        self.d3=layers.Dense(16, activation='relu')\n",
    "        self.d4=layers.Dense(outdim, activation='linear')\n",
    "        self.optimizer=tf.optimizers.Adam(0.001)\n",
    "\n",
    "    def call(self, x):\n",
    "        x=self.d1(x)\n",
    "        x=self.d2(x)\n",
    "        x=self.d3(x)\n",
    "        return self.d4(x)\n",
    "    \n",
    "class Agent():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size=2\n",
    "        self.action_size=3\n",
    "        self.eps=1.0\n",
    "        self.eps_decay=0.98\n",
    "        self.min_eps=0.1\n",
    "        self.batch_size=64\n",
    "        self.discount=0.99\n",
    "        self.learning_rate=0.001\n",
    "        self.memory=[]\n",
    "        self.model=DQN(self.input_size, self.action_size)\n",
    "\n",
    "    def update_eps(self):\n",
    "        self.eps=max(self.eps*self.eps_decay, self.min_eps)\n",
    "\n",
    "    def memory_update(self, n_state, action, reward, done, state):\n",
    "        self.memory.append((n_state, action, reward, done, state))\n",
    "\n",
    "    def get_act(self, state):\n",
    "        if np.random.rand() < self.eps:\n",
    "            return np.random.randint(0,3)\n",
    "        else:\n",
    "            return np.argmax(self.model.call(np.array([state])))\n",
    "        \n",
    "    def update_model(self):\n",
    "\n",
    "        if len(self.memory)<1000:\n",
    "            return\n",
    "\n",
    "        if len(self.memory)>20000:\n",
    "            del self.memory[0]\n",
    "\n",
    "        if len(self.memory)>self.batch_size:\n",
    "\n",
    "            mini_batch=random.sample(self.memory, self.batch_size)\n",
    "\n",
    "            n_states=np.array([x[0] for x in mini_batch])\n",
    "            actions=np.array([x[1] for x in mini_batch])\n",
    "            rewards=np.array([x[2] for x in mini_batch])\n",
    "            dones=np.array([x[3] for x in mini_batch])\n",
    "            states=np.array([x[4] for x in mini_batch])\n",
    "\n",
    "            q_val=self.model.call(states).numpy()\n",
    "            n_q_val=self.model.call(n_states).numpy()\n",
    "\n",
    "            targets=q_val.copy()\n",
    "            targets[np.arange(len(rewards)), actions]=rewards+self.discount*np.max(n_q_val, axis=1)*(1-dones)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_val=self.model.call(states)\n",
    "                loss=tf.keras.losses.mse(targets, q_val)\n",
    "\n",
    "            gradients=tape.gradient(loss, self.model.trainable_variables)\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "\n",
    "model=DQN(2,3)\n",
    "agent=Agent()\n",
    "agent.eps=0.01\n",
    "agent.model.load_weights('./save_model/model')\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    max_pos=state[0]\n",
    "    step=0\n",
    "    done=False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action=agent.get_act(state)\n",
    "        n_state, reward, done, trunc, _ = env.step(action)\n",
    "\n",
    "        if n_state[0]>max_pos:\n",
    "            max_pos=n_state[0]\n",
    "        if done:\n",
    "            reward=100\n",
    "        if action==2 and n_state[0]>state[0]:\n",
    "            reward=3\n",
    "        elif action==0 and n_state[0]<state[0]:\n",
    "            reward=3\n",
    "\n",
    "        state=n_state\n",
    "        step+=1\n",
    "\n",
    "    print(i+1, step)\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
