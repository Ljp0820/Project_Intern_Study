## 탐색과 활용 연습
~~뭐할까 고민 많이 해봤는데 제일 재미있는건 역시 코드 짜는게 아닐까요?(아님말고)~~<br>
주피터 노트북에서 권장(.ipynb)
#### 생각보다 많이 어려울 수도 있지만, 혼자힘으로 코드를 처음부터 끝까지 써내려가는 과정은 매우 특별한 경험입니다.
대부분의 경우 구글링, chat gpt와 같은 매체의 도움을 받거나. 완성된 코드를 읽어보고 이해하거나 하는 방법을 통해 공부합니다.<br>
주석도 없이, 필요한 라이브러리부터 자신이 생각해서 ```import```를 써내려가는 과정들 하나하나가 여러분의 코딩 실력을 키우는데 매우 큰 역할을 해줄겁니다.
```python
# 필요한 라이브러리

# 기본 parameter
time_step = 10000
slot_prob = [0.1, 0.15, 0.1, 0.65]
```
### Epsilon-Greedy

```python
# 일반 입실론 방법 데이터 컨테이너


# 입실론 값
eps = 0.7

# 아래 모든 과정들이 반복되어야합니다.

    # 1. 액션 선택
    # 탐색의 경우 랜덤한 선택
    # 활용의 경우 보상이 가장 큰 행동 선택
    # 그냥 누적 보상이 가장 큰 행동을 선택하는 경우에 초기에 운이 좋게 얻은 보상을 과대평가하게 됩니다.
    # 그러므로 평균 보상이 가장 큰 행동을 선택함으로 보상 편향을 극복 가능합니다.
    # Hint. 리스트의 요소 중 가장 큰 요소의 인덱스를 추출하는 방법에 대해 생각해보기.

    
    

    # 2. 보상 부여(누적 보상 업데이트)
    # 선택한 slot이 당첨되면 +1의 보상을, 그렇지 않다면 0의 보상 부여



    # 3. 평균 보상과 총 누적 보상율 업데이트
    # 평균 보상의 경우 보상의 평균입니다.(당연하게도)
    # 풀어서 쓰면 특정 행동의 평균 보상입니다.
    # Hint. 평균을 계산할 때 나눗셈을 하게 되는데, 분모가 0이 되지 않도록 예외 처리를 잘 합시다.
    

# 시각화까지...?(이건 안해도 됩니다...)


```

### Thomson Sampling

```python
# 톰슨 샘플링 컨테이너


# 반복문

    # 1. 액션 선택
    # 각 슬롯마다 베타 분포를 따르는 난수 추출
    # 베타 분포는 어떻게 구현할것인가...?
    # Hint. 구글링해서 코드를 보강해가는 과정도 매우 중요합니다.
    
    


    # 2. 보상 부여 및 베타 함수 파라미터 업데이트
    # 선택한 슬롯의 확률을 바탕으로 슬롯의 성공 여부 확인
    # Hint. 베타 함수의 파라미터들을 담는 컨테이너를 어떻게 구현할 것인지 생각해봅시다.



    # 3. 평균 보상과 총 누적 보상율 업데이트
   

# 전체 선택횟수 확인과 시각화
print(num_of_sel)
plt.plot(np.arange(1, time_step+1), total_reward_ts)
plt.show()
```

### UCB
혹시 두 개로 부족할까봐...
```python
# UCB 컨테이너


# 반복

    # UCB 점수 업데이트
    

    
    # UCB 점수를 바탕으로 행동 선택하기
    

    # 보상 부여
    
    
    # 평균 보상과 누적 보상
    

# 시각화
plt.plot(np.arange(1, time_step+1), total_reward_ucb)
plt.show()
```